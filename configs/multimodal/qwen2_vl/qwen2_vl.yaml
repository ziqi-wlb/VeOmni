model:
  model_path: qwen2vl-7b-instruct

data:
  train_path: sharegpt4v_pretrain
  chat_template: qwen2vl
  max_seq_len: 2048
  train_size: 80000000
  source_name: sharegpt4v_pretrain

train:
  output_dir: qwen2vl_sft
  data_parallel_mode: fsdp1
  wandb_project: qwen2vl
  wandb_name: qwen2vl
  rmpad: false
  rmpad_with_pos_ids: true
  ulysses_parallel_size: 1
  freeze_vit: false
  lr: 1.0e-5
  lr_decay_style: cosine
  num_train_epochs: 3
  micro_batch_size: 1
  global_batch_size: 32
  max_steps: 2000
